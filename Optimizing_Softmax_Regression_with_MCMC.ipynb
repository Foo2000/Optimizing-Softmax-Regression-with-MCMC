{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Softmax Regression with MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import emcee\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):                                        # define the softmax function\n",
    "            z -= np.max(z)                             # for numerical stability\n",
    "            return (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    \n",
    "def one_hot(y, C):                                     # define the one-hot encoding for labels\n",
    "            return (np.arange(C) == y[:, None]).astype(float)\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "    \n",
    "    def fit(self, x, y, optimizer, x_vali=None, y_vali=None):\n",
    "        if x_vali is None:\n",
    "            x_vali = x\n",
    "        if y_vali is None:\n",
    "            y_vali = y\n",
    "        if x_vali.ndim == 1:\n",
    "            x_vali = x_vali[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x_vali.shape[0]\n",
    "            x_vali = np.column_stack([x_vali,np.ones(N)])\n",
    "        \n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        N,D = x.shape\n",
    "        C = np.max(y) + 1\n",
    "        def gradient(x, y, w):                          # define the gradient function\n",
    "            yh = softmax(np.dot(x, w))                  # predictions\n",
    "            y = one_hot(y, C)                           # labels with one-hot encoding\n",
    "            N, D = x.shape\n",
    "            grad = np.dot(x.T, (yh - y))/N              # divide by N because cost is mean over N points\n",
    "            return grad\n",
    "        w0 = np.zeros((D, np.max(y) + 1))               # initialize the weights to 0, note that the dimension is D*C\n",
    "        self.w = optimizer.run(gradient, x, y, w0, x_vali, y_vali)      # run the optimizer to get the optimal weights\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        yh = softmax(np.dot(x,self.w))                  #predict output\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the output of the Softmax Regression is the probabilities for the label classes\n",
    "# so we need to convert the output into the label classes with the highest probabilities for the classification purpose\n",
    "\n",
    "def to_classlabel(z):                                   # convert the output class probabilities to be class labels\n",
    "    return z.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer 1: Bayesian inference with MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer 2: Gradient Descent (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(x, y, batch_size): \n",
    "    mini_batches = []\n",
    "    data = np.hstack((x, y[:,None]))\n",
    "    np.random.shuffle(data) \n",
    "    n_minibatches = math.ceil(data.shape[0] // batch_size) \n",
    "    data = np.array_split(data, n_minibatches)\n",
    "    mini_batches = []\n",
    "    for batch_data in data:\n",
    "        x_mini = batch_data[:, :-1]\n",
    "        y_mini = batch_data[:, -1]\n",
    "        mini_batches.append((x_mini, y_mini))\n",
    "    return mini_batches\n",
    "\n",
    "class ImprovedGradientDescent:\n",
    "    def __init__(self, learning_rate=0.1, max_iters=1e4, epsilon=1e-8, record_history=False, momentum=0.5, batch_size=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.record_history = record_history\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        if record_history:\n",
    "            self.w_history = []                         #to store the weight history for visualization\n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w, x_vali=None, y_vali=None):\n",
    "        if x_vali is None:\n",
    "            x_vali = x\n",
    "        if y_vali is None:\n",
    "            y_vali = y\n",
    "        grad = np.inf\n",
    "        grad_old = 0\n",
    "        t = 1\n",
    "        accuracy = 0\n",
    "        accuracy_best = 0\n",
    "        num_worse = 0\n",
    "        w_best = w\n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            mini_batches = create_mini_batches(x, y, self.batch_size) \n",
    "            for mini_batch in mini_batches: \n",
    "                x_mini, y_mini = mini_batch\n",
    "                grad = self.momentum*grad_old + (1-self.momentum)*gradient_fn(x_mini, y_mini, w)  # compute the gradient with present weight and momentum\n",
    "                w = w - self.learning_rate * grad       # weight update step\n",
    "                grad_old = grad                         # the current gradient is the old gradient for the next iteration\n",
    "            if self.record_history:\n",
    "                self.w_history.append(w)\n",
    "            accuracy = (to_classlabel(softmax(np.dot(x_vali, w))) == y_vali).astype(float).mean()\n",
    "            if accuracy > accuracy_best:\n",
    "                num_worse = 0\n",
    "                accuracy_best = accuracy\n",
    "                w_best = w\n",
    "            else:\n",
    "                num_worse += 1\n",
    "            if num_worse >= 20:\n",
    "                break\n",
    "            t += 1\n",
    "        w = w_best\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis 1: Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "iris_data = np.hstack((iris['data'],iris['target'][:,None]))\n",
    "np.random.shuffle(iris_data)\n",
    "\n",
    "train = iris_data[:(len(iris_data)*3//4)]\n",
    "train_x = train[:,:-1]\n",
    "train_y = train[:,-1].astype(int)\n",
    "\n",
    "test = iris_data[(len(iris_data)*3//4):]\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time is 0.1765596866607666\n",
      "The test accuracy is 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "regressor_iris_GD = SoftmaxRegression().fit(train_x,train_y, \n",
    "                                            ImprovedGradientDescent(learning_rate=0.01,momentum=0.9, batch_size=1))\n",
    "stop = time.time()\n",
    "train_time = stop - start\n",
    "test_yh = regressor_iris_GD.predict(test_x)\n",
    "test_accuracy = (to_classlabel(test_yh) == test_y).astype(float).mean()\n",
    "print(\"The training time is\", train_time)\n",
    "print(\"The test accuracy is\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
