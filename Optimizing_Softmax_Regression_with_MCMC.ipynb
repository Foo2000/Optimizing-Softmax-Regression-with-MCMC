{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Softmax Regression with MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import emcee\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):                                        # define the softmax function\n",
    "            z -= np.max(z)                             # for numerical stability\n",
    "            return (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    \n",
    "def one_hot(y, C):                                     # define the one-hot encoding for labels\n",
    "            return (np.arange(C) == y[:, None]).astype(float)\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "    \n",
    "    def fit(self, x, y, optimizer):\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        N,D = x.shape\n",
    "        C = np.max(y) + 1\n",
    "        w0 = np.zeros((D, C))               # initialize the weights to 0, note that the dimension is D*C\n",
    "        # optimize using MCMC\n",
    "        if isinstance(optimizer, BayesianInferenceMCMC):\n",
    "            def log_likelihood(w_flat, x, y):\n",
    "                w_og = w_flat.reshape(x.shape[1],np.max(y) + 1)\n",
    "                yh = softmax(np.dot(x, w_og))                  # predictions, i.e. the probabilities for each label class\n",
    "                log_like = np.sum(np.log(yh.max(axis=1)))   # log likelihood is the sum of the logs of all probabilities\n",
    "                \n",
    "                if math.isnan(log_like):\n",
    "                    global dotxw \n",
    "                    dotxw = np.dot(x, w_og)\n",
    "                    global nanyh \n",
    "                    nanyh = yh\n",
    "                    #return -np.inf\n",
    "                \n",
    "                return log_like\n",
    "            def log_prior(w_flat):\n",
    "                return 0.0                                  # no specific preference for the parameters\n",
    "            def log_post(w_flat, x, y):\n",
    "                return log_prior(w_flat) + log_likelihood(w_flat, x, y)\n",
    "            self.w = optimizer.run(log_post, x, y, w0)      # run the optimizer to get the optimal weights\n",
    "        # optimize using Gradient Descent\n",
    "        if isinstance(optimizer, ImprovedGradientDescent):\n",
    "            def gradient(x, y, w):                          # define the gradient function\n",
    "                yh = softmax(np.dot(x, w))                  # predictions\n",
    "                y = one_hot(y, C)                           # labels with one-hot encoding\n",
    "                N, D = x.shape\n",
    "                grad = np.dot(x.T, (yh - y))/N              # divide by N because cost is mean over N points\n",
    "                return grad\n",
    "            self.w = optimizer.run(gradient, x, y, w0)      # run the optimizer to get the optimal weights\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        yh = softmax(np.dot(x,self.w))                  #predict output\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the output of the Softmax Regression is the probabilities for the label classes\n",
    "# so we need to convert the output into the label classes with the highest probabilities for the classification purpose\n",
    "\n",
    "def to_classlabel(z):                                   # convert the output class probabilities to be class labels\n",
    "    return z.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer 1: Bayesian inference with MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianInferenceMCMC:\n",
    "    def __init__(self, visualization=False):\n",
    "        self.visualization = visualization\n",
    "            \n",
    "    def run(self, log_post, x, y, w):\n",
    "        num_iter = 2000\n",
    "        w_flat = w.reshape(-1)\n",
    "        ndim = len(w_flat) # number of parameters\n",
    "        nwalkers = 50\n",
    "        initial_pos = w_flat + 0.01 * np.random.randn(nwalkers, ndim)\n",
    "        sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(x, y))\n",
    "        state = sampler.run_mcmc(initial_pos, num_iter, progress=True)\n",
    "        sampler.reset()\n",
    "        sampler.run_mcmc(state, num_iter, progress=True)\n",
    "        w_flat = np.median(sampler.flatchain, axis=0)\n",
    "        w = w_flat.reshape(x.shape[1],np.max(y) + 1)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer 2: Gradient Descent (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(x, y, batch_size): \n",
    "    mini_batches = []\n",
    "    data = np.hstack((x, y[:,None]))\n",
    "    np.random.shuffle(data) \n",
    "    n_minibatches = math.ceil(data.shape[0] // batch_size) \n",
    "    data = np.array_split(data, n_minibatches)\n",
    "    mini_batches = []\n",
    "    for batch_data in data:\n",
    "        x_mini = batch_data[:, :-1]\n",
    "        y_mini = batch_data[:, -1]\n",
    "        mini_batches.append((x_mini, y_mini))\n",
    "    return mini_batches\n",
    "\n",
    "class ImprovedGradientDescent:\n",
    "    def __init__(self, learning_rate=0.1, max_iters=1e4, epsilon=1e-8, record_history=False, momentum=0.5, batch_size=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.record_history = record_history\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        if record_history:\n",
    "            self.w_history = []                         #to store the weight history for visualization\n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        grad = np.inf\n",
    "        grad_old = 0\n",
    "        t = 1\n",
    "        accuracy = 0\n",
    "        accuracy_best = 0\n",
    "        num_worse = 0\n",
    "        w_best = w\n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            mini_batches = create_mini_batches(x, y, self.batch_size) \n",
    "            for mini_batch in mini_batches: \n",
    "                x_mini, y_mini = mini_batch\n",
    "                grad = self.momentum*grad_old + (1-self.momentum)*gradient_fn(x_mini, y_mini, w)  # compute the gradient with present weight and momentum\n",
    "                w = w - self.learning_rate * grad       # weight update step\n",
    "                grad_old = grad                         # the current gradient is the old gradient for the next iteration\n",
    "            if self.record_history:\n",
    "                self.w_history.append(w)\n",
    "            accuracy = (to_classlabel(softmax(np.dot(x, w))) == y).astype(float).mean()\n",
    "            if accuracy > accuracy_best:\n",
    "                num_worse = 0\n",
    "                accuracy_best = accuracy\n",
    "                w_best = w\n",
    "            else:\n",
    "                num_worse += 1\n",
    "            if num_worse >= 20:\n",
    "                break\n",
    "            t += 1\n",
    "        w = w_best\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis 1: Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "iris_data = np.hstack((iris['data'],iris['target'][:,None]))\n",
    "np.random.shuffle(iris_data)\n",
    "\n",
    "train = iris_data[:(len(iris_data)*3//4)]\n",
    "train_x = train[:,:-1]\n",
    "train_y = train[:,-1].astype(int)\n",
    "\n",
    "test = iris_data[(len(iris_data)*3//4):]\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time is 0.13334107398986816\n",
      "The test accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "regressor_iris_GD = SoftmaxRegression().fit(train_x,train_y, \n",
    "                                            ImprovedGradientDescent(learning_rate=0.01,momentum=0.9, batch_size=1))\n",
    "stop = time.time()\n",
    "train_time = stop - start\n",
    "test_yh = regressor_iris_GD.predict(test_x)\n",
    "test_accuracy = (to_classlabel(test_yh) == test_y).astype(float).mean()\n",
    "print(\"The training time is\", train_time)\n",
    "print(\"The test accuracy is\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2000 [00:00<?, ?it/s]<ipython-input-2-ec91cddf4647>:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
      "  1%|▉                                                                              | 23/2000 [00:00<00:06, 303.44it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Probability function returned NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b731474063ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m regressor_iris_GD = SoftmaxRegression().fit(train_x,train_y, \n\u001b[0m\u001b[0;32m      3\u001b[0m                                             BayesianInferenceMCMC())\n\u001b[0;32m      4\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-ec91cddf4647>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, optimizer)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mlog_post\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mlog_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_flat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_post\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m# run the optimizer to get the optimal weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;31m# optimize using Gradient Descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImprovedGradientDescent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-e057eca2bd5c>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, log_post, x, y, w)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0minitial_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_flat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.01\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnwalkers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memcee\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnsembleSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnwalkers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_post\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_mcmc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_mcmc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\emcee\\ensemble.py\u001b[0m in \u001b[0;36mrun_mcmc\u001b[1;34m(self, initial_state, nsteps, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnsteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\emcee\\ensemble.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, initial_state, log_prob0, rstate0, blobs0, iterations, tune, skip_initial_state_check, thin_by, thin, store, progress)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                     \u001b[1;31m# Propose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m                     \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccepted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\emcee\\moves\\red_blue.py\u001b[0m in \u001b[0;36mpropose\u001b[1;34m(self, model, state)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;31m# Compute the lnprobs of the proposed position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mnew_log_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_log_prob_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;31m# Loop over the walkers and update them accordingly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\emcee\\ensemble.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[1;34m(self, coords)\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[1;31m# Check for log_prob returning NaN.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Probability function returned NaN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Probability function returned NaN"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "regressor_iris_GD = SoftmaxRegression().fit(train_x,train_y, \n",
    "                                            BayesianInferenceMCMC())\n",
    "stop = time.time()\n",
    "train_time = stop - start\n",
    "test_yh = regressor_iris_GD.predict(test_x)\n",
    "test_accuracy = (to_classlabel(test_yh) == test_y).astype(float).mean()\n",
    "print(\"The training time is\", train_time)\n",
    "print(\"The test accuracy is\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n",
      "[0.00000000e+000 0.00000000e+000 1.67660225e-309]\n",
      "1.6766022543462e-309\n",
      "[-1044.78584644  -425.75926478  -655.60185217 -1069.88175956\n",
      " -1094.5688852   -609.98090529 -1139.95182974  -754.06423108\n",
      "  -723.24604454  -777.28641417  -530.79167969  -459.17629926\n",
      "  -521.31348446  -538.13961189 -1082.54317771  -768.42539893\n",
      "  -533.20909575  -707.37078343 -1089.12429284  -555.06682266\n",
      " -1092.32705665  -694.51524709  -496.51318612  -619.61981674\n",
      " -1116.09292577  -716.020757   -1083.86374974  -567.79226281\n",
      "  -535.70265494 -1089.40036673  -564.81263004 -1078.60159946\n",
      " -1045.62514726  -555.87812084  -701.47452967 -1081.51982764\n",
      " -1067.61032119 -1058.87940215 -1096.63673341  -644.73060116\n",
      " -1004.55187948  -693.09317847  -671.96684623 -1095.90899812\n",
      " -1094.8254182   -628.08852298 -1071.91999796  -739.97067538\n",
      " -1103.8731001   -787.69754203  -713.58380004  -737.90282716\n",
      "  -512.51854341  -721.83344787 -1068.90974516  -734.34042658\n",
      "  -589.14172609  -668.11729262  -493.43044957 -1099.00019641\n",
      "  -780.46117534 -1018.28579841  -692.80703565  -441.20038494\n",
      " -1117.03533036  -524.45702863  -734.68052247  -617.12524734\n",
      "  -832.90489905 -1052.59390179  -637.57779736  -466.87540649\n",
      "  -624.83281632 -1059.71870296  -590.86947841  -631.59798071\n",
      "  -548.52011972  -517.00380769  -586.23457706  -583.46393377\n",
      "  -558.05753458  -565.00674823  -573.6699859   -500.37120148\n",
      "  -762.87871302  -695.44657254  -759.41840599 -1046.28039881\n",
      " -1118.81441836 -1078.50111309  -664.12918107 -1081.04862534\n",
      " -1044.06657289 -1059.06345141  -529.31199862  -501.60821062\n",
      "  -700.60561904 -1126.59447108 -1041.51906064  -750.11419109\n",
      "  -642.74530561  -743.53307596 -1068.36605912  -526.59255822\n",
      "  -669.69540787  -685.97945645 -1055.59307545 -1097.47603423\n",
      "  -706.73507277 -1046.5564727   -757.83022181  -555.7564864 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-243808745497>:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n"
     ]
    }
   ],
   "source": [
    "def softmax_1(z):                                        # define the softmax function\n",
    "            z -= np.max(z)                             # for numerical stability\n",
    "            return (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "def softmax_2(z):                                        # define the softmax function\n",
    "            z -= np.max(z)                             # for numerical stability\n",
    "            return (np.exp(z).T).T\n",
    "def softmax_3(z):                                        # define the softmax function\n",
    "            z -= np.max(z)                             # for numerical stability\n",
    "            return (np.sum(np.exp(z),axis=1)).T\n",
    "def softmax_4(z):                                        # define the softmax function\n",
    "            z -= np.max(z)                             # for numerical stability\n",
    "            return (z).T\n",
    "print(softmax_1(dotxw)[0])\n",
    "print(softmax_2(dotxw)[0])\n",
    "print(softmax_3(dotxw)[0])\n",
    "print(softmax_4(dotxw)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
